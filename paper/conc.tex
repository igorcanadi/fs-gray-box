From the results obtained in Section~\ref{sec:results}, we found that the behavior
of identical OSs running directly on a machine and in a virtual machine was 
nearly identical. This suggests that VMware Workstation I/O closely emulates 
the behavior in a real machine, except with some small overhead.

To summarize our results, we found the best size for random reads to be a multiple 
of the page size, 4KB, and that for large reads the size with the best throughput
was approximately 7MB by measuring the latency and throughput with respect to various
read sizes. We observed that the throughput behaved as we expected with decreasing 
gain in throughput as it approached the limit.

We also found that the maximum prefetch size was 128KB by performing many 512B
sequential reads. This also revealed the ramp-up growth of the readahead window
based on the read request behavior.

When investigating file cache, we measured file cache size both with and without a 
balloon array that was used to occupy half of the available memory. 
Our results proved that operating system both in the non-virtualized and virtualized environment 
adapts the file cache size to the amount of free memory available. Almost all of free memory was dedicated
to the file cache over the course of our experiments.

Finally, we studied the way ext4 addresses data blocks from inodes. By measuring the latency of one-byte appends, 
we proved that in ext4, the file size after which file system adds additional layer of indirection isn't fixed,
 but rather that it changes and depends on other factors. We proved our hypothesis only in host OS environment, we
measured access anomalies in VM that rendered our conclusions in that environment invalid.
